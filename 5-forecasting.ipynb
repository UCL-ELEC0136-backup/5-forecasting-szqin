{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJxk-VOYX4ve"
   },
   "source": [
    "# LAB 5: Time series forecasting\n",
    "\n",
    "This last LAB addresses the common problem of modelling time series data in order to make predictions about the future behaviour. \n",
    "More specifically, we will explore the [**London Santander Cycle data**](https://data.london.gov.uk/dataset/number-bicycle-hires) provided by Transport for London. \n",
    "\n",
    "As reported on the TfL website, the dataset includes the total number of hires of the Santander Cycle Hire Scheme, by day, month and year for each day since the launch on 30 July 2010.\n",
    "\n",
    "For the purposes of this LAB, we are interested only in the daily data (i.e. columns A and B).\n",
    "\n",
    "![TFLdatapreview](tfl_data_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHptbA8sX4vf"
   },
   "source": [
    "## Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1638187155294,
     "user": {
      "displayName": "Eduardo Pignatelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXGC18_334aroRRv-7MAnrp8DbSMLZsnM7MllF=s64",
      "userId": "05381439510125166712"
     },
     "user_tz": 0
    },
    "id": "c06KfGfhX4vf"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from calendar import day_abbr, month_abbr, mdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuAQECYBX4vg"
   },
   "source": [
    "## Load data and plot\n",
    "\n",
    "The dataset contains two sheets: `Metadata` and `Data`. Therefore, we need to tell Pandas which one we want to look at via the `sheet_name` attribute of the `read_excel` method. Moreover, since there is no need to load data that we are not interested in, it is recommended to also specify the columns that we want to use.\n",
    "\n",
    "**NOTE** the timestamp (which is `Day` in the Excel file) is renamed `datetime` due to a common practice in Pandas. Also, we are selecting the dates from the 1st of Jan 2011 because the climate data that we will be using later in the LAB is from this date onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_2R5zcRX4vg"
   },
   "outputs": [],
   "source": [
    "def load_tfl_cycle_data():\n",
    "    cycle_df = pd.read_csv('./tfl-daily-cycle-hires.csv').iloc[:, 0:2]\n",
    "    cycle_df = cycle_df.rename(columns={'Day':'datetime'})\n",
    "    cycle_df['datetime'] = pd.to_datetime(cycle_df['datetime'])\n",
    "    cycle_df = cycle_df.set_index('datetime', drop=True)\n",
    "    cycle_df = cycle_df.loc['2011-01-01':, :]\n",
    "    \n",
    "    return cycle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DViYmXvZX4vg"
   },
   "outputs": [],
   "source": [
    "cycle_df = load_tfl_cycle_data()\n",
    "cycle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZLTU0TKX4vl"
   },
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "#### Plot the cycle data time series ####\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9MW6rRVX4vl"
   },
   "source": [
    "## 1. Exploratory data analysis\n",
    "\n",
    "Let's now explore the properties of the dataset. Time series data normally comprises of three main components:\n",
    "- Trend represents the overall tendency of the data to increase or decrease over time.\n",
    "- Seasonality relates to the presence of recurrent patterns that appear after regular intervals (like seasons).\n",
    "- Random noise is often hard to explain and represents all those changes in the data that seem unexpected. Sometimes sudden changes are related to fixed or predictable events (i.e., public holidays).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_f99ZH_X4vl"
   },
   "source": [
    "### 1.1 Explore seasonal cycles using a 30-day rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1sl6WOiX4vl"
   },
   "outputs": [],
   "source": [
    "seasonal_cycle = cycle_df.rolling(window=30, center=True).mean().groupby(cycle_df.index.dayofyear).mean()\n",
    "q25 = cycle_df.rolling(window=30, center=True).mean().groupby(cycle_df.index.dayofyear).quantile(0.25)\n",
    "q75 = cycle_df.rolling(window=30, center=True).mean().groupby(cycle_df.index.dayofyear).quantile(0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBxW3itTX4vl"
   },
   "outputs": [],
   "source": [
    "ndays_m = mdays.copy()\n",
    "ndays_m[2] = 29\n",
    "ndays_m = np.cumsum(ndays_m)\n",
    "month_ticks = month_abbr[1:]\n",
    "f, ax = plt.subplots(figsize=(10,7)) \n",
    "\n",
    "seasonal_cycle.plot(ax=ax, lw=2, color='b', legend=False)\n",
    "ax.fill_between(seasonal_cycle.index, q25.values.ravel(), q75.values.ravel(), color='b', alpha=0.3)\n",
    "ax.set_xticks(ndays_m + 15)\n",
    "ax.grid(ls=':')\n",
    "ax.set_xlabel('Month', fontsize=15)\n",
    "ax.set_ylabel('Number of Santander cycles hires', fontsize=15);\n",
    "ax.set_xlim(0, 365)\n",
    "ax.set_ylim(10000, 40000)\n",
    "[l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "[l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "ax.set_title('30 days running average hourly cycling counts', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb59LtMoX4vl"
   },
   "source": [
    "### 1.2 Explore dependency on year and month via carpet plot/heatmap\n",
    "\n",
    "Heatmaps (also called carpet plots) can give us useful information about the structure of the data. Pandas provides very handy functions to explore relevant dependencies. For example, here we show how the mean Number of Santander Cycles Hires varies as a function of the year and month. \n",
    "\n",
    "Although the number of hires seems to be increasing over time, this increase is not monotonic and probably depends on other factors as well. On the other hand, the carpet plot confirms the general trend shown before, with a higher usage of rented bikes over the warmer months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4ZZC5vKX4vm"
   },
   "outputs": [],
   "source": [
    "month_year = cycle_df.copy()\n",
    "month_year.loc[:,'year'] = month_year.index.year\n",
    "month_year.loc[:,'month'] = month_year.index.month\n",
    "month_year = month_year.groupby(['year','month']).mean().unstack()\n",
    "month_year.columns = month_year.columns.droplevel(0)\n",
    "\n",
    "month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbxcAEYSX4vm"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "sns.heatmap(month_year, ax=ax, cmap=plt.cm.viridis, cbar_kws={'boundaries':np.arange(10000,45000,5000)})\n",
    "\n",
    "cbax = f.axes[1]\n",
    "[l.set_fontsize(13) for l in cbax.yaxis.get_ticklabels()]\n",
    "cbax.set_ylabel('Santander cycles hires', fontsize=13)\n",
    "\n",
    "[ax.axhline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 7)]\n",
    "[ax.axvline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 24)];\n",
    "\n",
    "ax.set_title('Santander cycles hires per year and month', fontsize=16)\n",
    "\n",
    "[l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "[l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "ax.set_xlabel('Month', fontsize=15)\n",
    "ax.set_ylabel('Year', fontsize=15)\n",
    "ax.set_yticklabels(np.arange(2011, 2021, 1), rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzS2QO0AX4vm"
   },
   "source": [
    "### 1.3 Explore dependency on day of the week and month via carpet plot/heatmap\n",
    "**TODO** Produce a new heatmap showing the dependency on the day of the week and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-y3NalYX4vm"
   },
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "# Explore dependency on day of the week and month #\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPRdvM08X4vm"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "sns.heatmap(month_day, ax = ax, cmap=plt.cm.viridis, cbar_kws={'boundaries':np.arange(10000,45000,5000)})\n",
    "\n",
    "cbax = f.axes[1]\n",
    "[l.set_fontsize(13) for l in cbax.yaxis.get_ticklabels()]\n",
    "cbax.set_ylabel('Santander cycles hires', fontsize=13)\n",
    "\n",
    "[ax.axhline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 7)]\n",
    "[ax.axvline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 24)];\n",
    "\n",
    "ax.set_title('Santander cycles hires per day of the week and month', fontsize=16)\n",
    "\n",
    "[l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "[l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "ax.set_xlabel('Month', fontsize=15)\n",
    "ax.set_ylabel('Day of the week', fontsize=15)\n",
    "ax.set_yticklabels(day_abbr[0:7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FxbMq3fX4vn"
   },
   "source": [
    "### 1.4 Explore weekdays and weekends trends\n",
    "\n",
    "**TODO** We can also explore deeper trends by looking at differences between weekdays and weekends.\n",
    "\n",
    "**HINT** Pandas DateTime indexes have a built-in method that extracts the day of week (`DataFrame.index.dayofweek`), where 0 is Monday and 6 is Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0UXxZ9yX4vn"
   },
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "## Extract weekdays and weekends trends ##\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-MH-fuiX4vn"
   },
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "## Plot weekdays and weekends general trends along with the 25% and 75% IQR to have a sense of the variation over time ##\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGZFh3F8X4vn"
   },
   "source": [
    "## 2. Forecasting using Facebook Prophet\n",
    "\n",
    "Nowadays, there are many models for the predictive analysis of time series data such as ARIMA (**A**uto-**R**egressive **I**ntegrated **M**oving **A**verage model), Auto-Regressive models, Exponential Smoothing, LSTM (**L**ong **S**hort **T**erm **M**emory), etc. \n",
    "\n",
    "Here, we will show how to use a novel approach based on the *Facebook Prophet* library. Fbprophet implements a Generalized Additive Model, and models a time-series as the sum (or multiplication) of different components (trends, periodic components, holidays and special events) allowing to incorporate additional regressors taken from outer sources. \n",
    "The main reference is [Taylor and Letham, 2017](https://peerj.com/preprints/3190.pdf)\n",
    "\n",
    "See also [here](https://research.fb.com/prophet-forecasting-at-scale/) for the official announcement and [here](https://facebook.github.io/prophet/docs/quick_start.html#r-api) for the full documentation. \n",
    "\n",
    "The easiest way to install Prophet is through conda-forge. Open the Anaconda Prompt and type: `conda install -c conda-forge fbprophet`.\n",
    "\n",
    "As largely explained in the quick start webpage, Prophet follows the `sklearn` model API. Hence we just need create an instance of the Prophet class and then call its `fit` and `predict` methods.\n",
    "\n",
    "The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCEct0PYX4vn"
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AQ0CNCEX4vn"
   },
   "source": [
    "### 2.1 Base model\n",
    "\n",
    "We first prepare the data in order to be ingested by Prophet. Therefore, we must change the name of the datetime column to `ds` and the target feature to `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhLJAV2cX4vn"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, target_feature): \n",
    "    \"\"\"\n",
    "    prepare the data for ingestion by fbprophet: \n",
    "    see: https://facebook.github.io/prophet/docs/quick_start.html\n",
    "    \"\"\"\n",
    "    new_data = data.copy()\n",
    "    new_data.reset_index(inplace=True)\n",
    "    new_data = new_data.rename({'datetime':'ds', '{}'.format(target_feature):'y'}, axis=1)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVQ_jMhPX4vn"
   },
   "outputs": [],
   "source": [
    "cycle_df_new = prepare_data(data=cycle_df, target_feature='Number of Bicycle Hires')\n",
    "cycle_df_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ7rxqmDX4vn"
   },
   "source": [
    "#### 2.1.1 The holidays package\n",
    "\n",
    "Knowing when holidays and special events take place is often crucial when modelling time-series data. Here we make use of the `holidays` [package](https://github.com/dr-prodigy/python-holidays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3u_rpHZX4vn"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "holidays_df = pd.DataFrame([], columns = ['ds','holiday'])\n",
    "ldates = []\n",
    "lnames = []\n",
    "for date, name in sorted(holidays.England(years=np.arange(2011, 2020 + 1)).items()):\n",
    "    ldates.append(date)\n",
    "    lnames.append(name)\n",
    "    \n",
    "ldates = np.array(ldates)\n",
    "lnames = np.array(lnames)\n",
    "holidays_df.loc[:,'ds'] = ldates\n",
    "holidays_df.loc[:,'holiday'] = lnames\n",
    "holidays_df.holiday.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QouYWVV_X4vo"
   },
   "outputs": [],
   "source": [
    "holidays_df.loc[:,'holiday'] = holidays_df.loc[:,'holiday'].apply(lambda x : x.replace(' (Observed)',''))\n",
    "holidays_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubQ4hvTpX4vo"
   },
   "source": [
    "#### 2.1.2 Train test split and model fit\n",
    "\n",
    "We have decided to use the data up to the 31st of July to train the model and the rest (last 2 months) as the holdout test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJQS9dnkX4vo"
   },
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    \n",
    "    \n",
    "    train = data.set_index('ds').loc[:'2020-07-31', :].reset_index()\n",
    "    test = data.set_index('ds').loc['2020-08-01':, :].reset_index()\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpyX01-gX4vo"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data=cycle_df_new)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD3IkhtiX4vo"
   },
   "source": [
    "We now create a Prophet instance. The model has several parameters, the most important ones being the seasonalities. Here we set all the seasonalities but the daily to True (the data is on a daily basis, therefore we can't model in-day cycles).\n",
    "\n",
    "Here we instantiate the simplest Prophet model, but you can set other parameters such as the prior scales for each component of your time-series, as well as the number of Fourier series to use to model the cyclic components.\n",
    "Normally, larger prior scales and a higher order Fourier series will make the model more flexible, but at the cost of a potential overfit. Setting the hyperparameters is of crucial importance and it is normally done via cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87qzEqtgX4vo"
   },
   "outputs": [],
   "source": [
    "m = Prophet(holidays=holidays_df,\n",
    "            seasonality_mode='multiplicative',\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7OaSyfKX4vo"
   },
   "outputs": [],
   "source": [
    "m.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ckoO94ZX4vo"
   },
   "source": [
    "The method `make_future_dataframe` creates an extension to the training data in the \"future\". Here our future is obviously the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLMN7hCDX4vo"
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=len(test), freq='1D')\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh4IkhLNX4vp"
   },
   "source": [
    "The `predict` method produces a comprehensive DataFrame comprising of all the modelled components of the time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0AlxYNkX4vp"
   },
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tmXy6wpX4vp"
   },
   "outputs": [],
   "source": [
    "f = m.plot_components(forecast, figsize=(12, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t4aWEWvX4vp"
   },
   "outputs": [],
   "source": [
    "def make_predictions_df(forecast, data_train, data_test): \n",
    "    \"\"\"\n",
    "    Function to convert the output Prophet dataframe to a datetime index and append the actual target values at the end\n",
    "    \"\"\"\n",
    "    forecast.index = pd.to_datetime(forecast.ds)\n",
    "    data_train.index = pd.to_datetime(data_train.ds)\n",
    "    data_test.index = pd.to_datetime(data_test.ds)\n",
    "    data = pd.concat([data_train, data_test], axis=0)\n",
    "    forecast.loc[:,'y'] = data.loc[:,'y']\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "def plot_predictions(forecast, start_date):\n",
    "    \"\"\"\n",
    "    Function to plot the predictions \n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    train = forecast.loc[start_date:'2020-07-31',:]\n",
    "    ax.plot(train.index, train.y, 'ko', markersize=3)\n",
    "    ax.plot(train.index, train.yhat, color='steelblue', lw=0.5)\n",
    "    ax.fill_between(train.index, train.yhat_lower, train.yhat_upper, color='steelblue', alpha=0.3)\n",
    "    \n",
    "    test = forecast.loc['2020-08-01':,:]\n",
    "    ax.plot(test.index, test.y, 'ro', markersize=3)\n",
    "    ax.plot(test.index, test.yhat, color='coral', lw=0.5)\n",
    "    ax.fill_between(test.index, test.yhat_lower, test.yhat_upper, color='coral', alpha=0.3)\n",
    "    ax.axvline(forecast.loc['2020-08-01', 'ds'], color='k', ls='--', alpha=0.7)\n",
    "\n",
    "    ax.grid(ls=':', lw=0.5)\n",
    "    \n",
    "    return f, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu2lwZT1X4vp"
   },
   "source": [
    "Since the target must be strictly positive (but our model doesn't know it), we clip predictions and confidence bands to have a lower value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKjmVJTAX4vp"
   },
   "outputs": [],
   "source": [
    "result = make_predictions_df(forecast, train, test)\n",
    "result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALlOtkuRX4vp"
   },
   "outputs": [],
   "source": [
    "f, ax = plot_predictions(result, '2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s50Gp_GVX4vp"
   },
   "source": [
    "At a first glance we can see that the model is not performing well. That is, it can capture the overall trend of the data in terms of annual and weekly oscillations, but is not able to generalise well on unseen data. We can spot the poor performance by employing a joint plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZQfdaAhX4vp"
   },
   "outputs": [],
   "source": [
    "def create_joint_plot(forecast, x='yhat', y='y', title=None): \n",
    "\n",
    "    g = sns.jointplot(x='yhat', y='y', data=forecast, kind=\"reg\", color=\"b\")\n",
    "    g.fig.set_figwidth(8)\n",
    "    g.fig.set_figheight(8)\n",
    "    \n",
    "    ax = g.fig.axes[1]\n",
    "    if title is not None: \n",
    "        ax.set_title(title, fontsize=16)\n",
    "\n",
    "    ax = g.fig.axes[0]\n",
    "    ax.text(5000, 60000, \"R = {:+4.2f}\".format(forecast.loc[:,['y','yhat']].corr().iloc[0,1]), fontsize=16)\n",
    "    ax.set_xlabel('Predictions', fontsize=15)\n",
    "    ax.set_ylabel('Observations', fontsize=15)\n",
    "    ax.set_xlim(0, 80000)\n",
    "    ax.set_ylim(0, 80000)\n",
    "    ax.grid(ls=':')\n",
    "    [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "    [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()];\n",
    "\n",
    "    ax.grid(ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63ZB6GsBX4vp"
   },
   "outputs": [],
   "source": [
    "create_joint_plot(result.loc[:'2020-07-31', :], title='Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBEeaN5NX4vq"
   },
   "outputs": [],
   "source": [
    "create_joint_plot(result.loc['2020-08-01':, :], title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyaek_eWX4vq"
   },
   "source": [
    "## 2.2 Model with extra-regressors: incorporating the effect of climate conditions\n",
    "\n",
    "It is clear that the data is not enough to model the temporal evolution of the number of Santander Cycles Hires. Data scientists should be able to gather external data that they think might be useful for the problem at hand. In this case, is quite clear that weather conditions should strongly influence the amount of Londoners renting bikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABW-yELhX4vq"
   },
   "outputs": [],
   "source": [
    "df_weather = pd.read_pickle('LondonWeatherHourly.pkl')\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl1PJZiKX4vq"
   },
   "source": [
    "The weather data from a station located in NW3 is recorded every hour and reports the average temperature, wind speed, rain and cloud cover. For our purposes, we have to resample the data on a daily basis. Be careful, though. Bike rentals mostly happen during daytime, when people commute between home and work or other places. Therefore, we must label they daytime hours before resampling.\n",
    "\n",
    "**TODO** We provide a simple function that labels as 1 the hours between 6 am and 8 pm and 0 otherwise. Use the `.apply` method in Pandas to create a new column in the DataFrame called `is_daytime_hour`. Then drop all the rows where the label is 0, because we will not consider those in the resampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTss1RWIX4vq"
   },
   "outputs": [],
   "source": [
    "def is_daytime_hour(datetime):\n",
    "    if 6 <= datetime.hour <= 20:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMlXKvzxX4vq"
   },
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "\n",
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rEh71lDX4vq"
   },
   "source": [
    "**TODO** Now, we can resample the data on a daily basis using the well-known `.resample` method in Pandas. \n",
    "\n",
    "**NOTE** Features like temperature, wind speed and cloud cover can be averaged over the day when resampling. Whereas the precipation is cumulative over the day and taking the average value would not be a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufc2UZjzX4vq"
   },
   "outputs": [],
   "source": [
    "###### TODO ######\n",
    "# Resample the data on a daily basis. Check that the length of the resampled DataFrame matches the \n",
    "# length of the Cycle DataFrame (3561 rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD4p3KNuX4vq"
   },
   "source": [
    "**TODO** Concatenate the `cycle_df_new` DataFrame obtained in 2.1 with the resampled weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XllayDNHX4vr"
   },
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "# Concatenate the old dataframe with the additional regressors #\n",
    "\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gIfo9-1X4vr"
   },
   "source": [
    "**TODO** Split the new data into training and test sets. Then instantiate a new Prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5bT_oX2X4vr"
   },
   "outputs": [],
   "source": [
    "### TODO 1 ###\n",
    "## Train-test split\n",
    "\n",
    "\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0g21kpfX4vr"
   },
   "outputs": [],
   "source": [
    "#### TODO 2 ####\n",
    "## Instantiate a new Prophet model\n",
    "\n",
    "\n",
    "###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOBOQp5PX4vr"
   },
   "source": [
    "**TODO** We now want to inform the model that extra-regressors are to be added. The `.add_regressor` method is a simple way of adding extra regressors. See [documentation](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#additional-regressors) for the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOg--m9tX4vr"
   },
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "# Add the 4 additional regressors to the model, namely 'temp', 'wind_speed', 'rain_1h' and 'clouds_all'\n",
    "\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNJ9SykgX4vr"
   },
   "source": [
    "**TODO** Fit the model to the new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04pDMSbDX4vr"
   },
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "# Fit the model \n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoZ2trV8X4vr"
   },
   "source": [
    "**TODO** Make the future DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hIb4UEpX4vr"
   },
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "# Make the future DataFrame\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1ln2s3SX4vs"
   },
   "source": [
    "**TODO** The `make_future_dataframe` method by default creates only the `ds` column. Since there are additional regressors we must append them on it. Create a new df called `futures` which includes the 4 additional regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEjit414X4vs"
   },
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "#Create a dataframe called futures which includes the 4 additional regressors\n",
    "\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enWrLLyJX4vs"
   },
   "source": [
    "**TODO** Now we can forecast using the `predict` method as before and plot the componenents of the new model. As expected, you will notice that the extra-regressors are quite influential on the overall trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UMQB9E6X4vs"
   },
   "outputs": [],
   "source": [
    "### TODO Forecast ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCE7RE94X4vs"
   },
   "outputs": [],
   "source": [
    "### TODO plot components ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-i-qDhhX4vs"
   },
   "source": [
    "**TODO** Produce a plot of the predictions. Remember to clip the `yhat`, `yhat_lower` and `yhat_upper` columns to 0. You will notice that the extra regressors help locate additional fluctuations in the data. However, if you produce a joint plot you will see that the performance on the test set is still not enough (although strongly improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtEZZWRsX4vs"
   },
   "outputs": [],
   "source": [
    "## TODO produce predictions and plot ###\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw_Cu4gkX4vs"
   },
   "outputs": [],
   "source": [
    "create_joint_plot(result.loc[:'2020-07-31', :], title='Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLoBLXsZX4vs"
   },
   "outputs": [],
   "source": [
    "create_joint_plot(result.loc['2020-08-01':, :], title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OVP_ruDX4vt"
   },
   "source": [
    "## 2.3 Incorporate the effect of a pandemic\n",
    "\n",
    "Choosing the right features and hyperparameters can be more an art than a science. We here want to show you a third possible model that incorporates a yearly seasonality if a specific year is affected by a pandemic or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CV05ocvYX4vt"
   },
   "outputs": [],
   "source": [
    "def is_pandemic_affected(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return date.year == 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsI6Z6LVX4vt"
   },
   "outputs": [],
   "source": [
    "df_with_weather_covid = df_with_weather.copy()\n",
    "df_with_weather_covid['pandemic_affected'] = df_with_weather_covid['ds'].apply(is_pandemic_affected)\n",
    "df_with_weather_covid['not_pandemic_affected'] = ~df_with_weather_covid['ds'].apply(is_pandemic_affected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_wp0x7WX4vt"
   },
   "outputs": [],
   "source": [
    "df_with_weather_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zdikdy8WX4vt"
   },
   "outputs": [],
   "source": [
    "train_weather_covid, test_weather_covid = train_test_split(df_with_weather_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOcZ-qT9X4vt"
   },
   "source": [
    "Instantiate new Prophet model with yearly_seasonality set to `False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKt8XVyzX4vt"
   },
   "outputs": [],
   "source": [
    "m = Prophet(holidays=holidays_df, \n",
    "            seasonality_mode='multiplicative',\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB6UoEq8X4vt"
   },
   "outputs": [],
   "source": [
    "m.add_regressor('temp', mode='multiplicative')\n",
    "m.add_regressor('wind_speed', mode='multiplicative')\n",
    "m.add_regressor('rain_1h', mode='multiplicative')\n",
    "m.add_regressor('clouds_all', mode='multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX3dwOGTX4vt"
   },
   "outputs": [],
   "source": [
    "m.add_seasonality(name='pandemic_affected', period=365, fourier_order=3, mode='multiplicative', condition_name='pandemic_affected')\n",
    "m.add_seasonality(name='not_pandemic_affected', period=365, fourier_order=3, mode='multiplicative', condition_name='not_pandemic_affected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NhYEtwzX4vt"
   },
   "outputs": [],
   "source": [
    "m.fit(train_weather_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffEjgDk3X4vt"
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=len(test_weather_covid), freq='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxsGEO8TX4vu"
   },
   "outputs": [],
   "source": [
    "futures = pd.concat([future, df_weather_resampled.loc[:, ['temp', 'wind_speed', 'rain_1h', 'clouds_all']].reset_index()], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AosG87xwX4vu"
   },
   "outputs": [],
   "source": [
    "futures['pandemic_affected'] = futures['ds'].apply(is_pandemic_affected)\n",
    "futures['not_pandemic_affected'] = ~futures['ds'].apply(is_pandemic_affected)\n",
    "futures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbhyE5MFX4vu"
   },
   "outputs": [],
   "source": [
    "forecast = m.predict(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JzGv4i0X4vu"
   },
   "outputs": [],
   "source": [
    "result = make_predictions_df(forecast, train_weather_covid, test_weather_covid)\n",
    "result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqvW1FAOX4vu"
   },
   "outputs": [],
   "source": [
    "f, ax = plot_predictions(result, '2019-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThFxDPyrX4vu"
   },
   "outputs": [],
   "source": [
    "create_joint_plot(result.loc[:'2020-07-31', :], title='Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4RNifJoX4vu"
   },
   "outputs": [],
   "source": [
    "create_joint_plot(result.loc['2020-08-01':, :], title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bouj4Zi4X4vu"
   },
   "source": [
    "Although the performance has not improved in terms of $R^2$, we can see that the information about the COVID-19 pandemic made the model able to discern that in 2020 the behaviour is different from the previous years. In fact, by looking at the `pandemic_affected` component we can observe several things:\n",
    "\n",
    "- there is an unexpected and sudden drop in March and April, not present in the previous years.\n",
    "- strong increase in the warmer months. This can be related to multiple causes  such as the particularly sunny summer, the relaxation of restrictions across the UK, people avoiding public transport, and the major changes to the cycling infrastructure made in London.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmrhmNr5X4vv"
   },
   "outputs": [],
   "source": [
    "f = m.plot_components(forecast, figsize=(12, 18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQDj_lFsX4vv"
   },
   "source": [
    "# 3. Additional analyses (up to the student)\n",
    "\n",
    "This final LAB is meant to stimulate your fantasy in preparation for the final assignment. In fact, we have purposedly shown you the basic usage of a powerful tool for time-series analysis and forecasting, and we would like you to play with the model's parameters and/or show if any improvements can be achieved by feeding the model with additional information. \n",
    "\n",
    "Here some suggestions:\n",
    "\n",
    "- try to play with the model's components. How does the predictions/performance change by setting the `mode` parameter to additive? \n",
    "- can you fine tune the prior scales in order to reduce the overfitting issue?\n",
    "- Prophet detects changepoints by first specifying a large number of potential time points at which the rate is allowed to change. It then puts a sparse prior on the magnitudes of the rate changes (equivalent to L1 regularization) and then uses as few of them as possible. However, changepoints can be customised. Feel free to play with them by following the [documentation](https://facebook.github.io/prophet/docs/trend_changepoints.html#automatic-changepoint-detection-in-prophet). \n",
    "- can you identify additional regressors or custom seasonalities that might help modelling the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CTaosbAX4vv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAB5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
